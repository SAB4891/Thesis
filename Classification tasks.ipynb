{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification tasks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### TASK 1 #########################\n",
    "\n",
    "def train_test_split(X, y):\n",
    "    I = (X.shape[0]//3)*2\n",
    "    X_train = X[:I, :]\n",
    "    y_train = y[:I]\n",
    "    X_val = X[I:,:]\n",
    "    y_val = y[I:]\n",
    "    return X_train, X_val, y_train, y_val\n",
    "\n",
    "import numpy as np\n",
    "def classifier(reps):\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    model = LogisticRegression(solver='sag', random_state=None,  multi_class='multinomial', warm_start=False, max_iter = 2000)\n",
    "    X = np.load(reps+\"_features.npy\")\n",
    "    y = np.load(reps+\"_phonemes.npy\")\n",
    "   \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    \n",
    "    model.fit(X_train,y_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    val_score = 1-model.score(X_val, y_val)\n",
    "    \n",
    "    W = np.load(\"test_\"+reps+\"_features.npy\")\n",
    "    W = scaler.transform(W)\n",
    "    z = np.load(\"test_\"+reps+\"_phonemes.npy\")\n",
    "    \n",
    "    test_error_rate = 1-model.score(W, z)\n",
    "    global_error_rates[reps]= (val_score, test_error_rate)\n",
    "    return(val_score, test_error_rate)\n",
    "\n",
    "print(classifier())\n",
    "\n",
    "###VIZUALIZATION####\n",
    "import pandas as pd \n",
    "representations = ['mfcc','conv', 'rec0', 'rec1', 'rec2', 'rec3']\n",
    "validation_errors = [0.5852624655131461,  0.6331695017448977, 0.42787451352980943, 0.4196159397781879, \n",
    "                     0.43357512196013226, 0.44196159397781876 ]\n",
    "test_errors =[0.6081288343558282, 0.6614263803680982, 0.5663343558282208, 0.5617331288343559, \n",
    "              0.5648006134969326, 0.5751533742331288 ]\n",
    "\n",
    "dataset = {'representations':representations, 'validation_errors': validation_errors, 'test_errors': test_errors}\n",
    "\n",
    "df = pd.DataFrame(data = dataset)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "sns.set_style(\"white\", {'legend.frameon':False})\n",
    "g1 = sns.pointplot(x=\"representations\", y=\"validation_errors\", color='blue', scale =0.7, linestyles ='--',data=df)\n",
    "g2 = sns.pointplot(x=\"representations\", y=\"test_errors\", color='red', scale =0.7,linestyles ='--', data=df)\n",
    "\n",
    "blue_patch = mpatches.Patch(color='red', label='test data error')\n",
    "red_patch = mpatches.Patch(color='blue', label='train data validation error')\n",
    "plt.legend(handles=[red_patch, blue_patch], frameon = False, loc='best', bbox_to_anchor=(1, 0., 0.5, 0.5))\n",
    "sns.despine()\n",
    "plt.xlabel(\"representations\")\n",
    "plt.ylabel(\"error rates\")\n",
    "plt.title(\"errors per layer\")\n",
    "print(plt.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### TASK 2 #######################\n",
    "\n",
    "def train_test_split(X, y):\n",
    "    I = (X.shape[0]//3)*2\n",
    "    X_train = X[:I, :]\n",
    "    y_train = y[:I]\n",
    "    X_val = X[I:,:]\n",
    "    y_val = y[I:]\n",
    "    return X_train, X_val, y_train, y_val\n",
    "\n",
    "import numpy as np\n",
    "def classifier(reps):\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    model = LogisticRegression(solver='sag', random_state=None,  multi_class='multinomial', warm_start=False, max_iter = 2000)\n",
    "    X = np.load(reps+\"_features.npy\")\n",
    "    y = np.load(reps+\"_phonemes.npy\")\n",
    "   \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    \n",
    "    model.fit(X_train,y_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    \n",
    "\n",
    "    W = list_separated(reps)['original'][1]\n",
    "    W = scaler.transform(W)\n",
    "    z = list_separated(reps)['original'][0]\n",
    "    ori_err = 1-model.score(W, z)\n",
    "        \n",
    "    W1 = list_separated(reps)['manipulated'][1]\n",
    "    W1 = scaler.transform(W1)\n",
    "    z1 = list_separated(reps)['manipulated'][0]\n",
    "    mani_err = 1-model.score(W1, z1)\n",
    "    \n",
    "    return(ori_err, mani_err)\n",
    "\n",
    "###VIZUALIZATION####\n",
    "\n",
    "import pandas as pd \n",
    "representations = ['mfcc','conv', 'rec0', 'rec1', 'rec2', 'rec3']\n",
    "original_errors = [0.653061224489796, 0.6734693877551021, 0.5510204081632653, 0.4897959183673469, 0.40816326530612246, 0.4897959183673469]\n",
    "manipulated_errors =[0.9387755102040817, 0.9591836734693877, 1.0, 0.9591836734693877,0.9591836734693877, 0.9387755102040817]\n",
    "\n",
    "dataset = {'representations':representations, 'original_errors': original_errors, 'manipulated_errors': manipulated_errors}\n",
    "\n",
    "df = pd.DataFrame(data = dataset)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "sns.set_style(\"white\", {'legend.frameon':False})\n",
    "g1 = sns.pointplot(x=\"representations\", y=\"original_errors\", color='blue', scale =0.7, linestyles ='--',data=df)\n",
    "g2 = sns.pointplot(x=\"representations\", y=\"manipulated_errors\", color='red', scale =0.7,linestyles ='--', data=df)\n",
    "\n",
    "blue_patch = mpatches.Patch(color='red', label='manipulated pho. errors')\n",
    "red_patch = mpatches.Patch(color='blue', label='original pho. errors')\n",
    "plt.legend(handles=[red_patch, blue_patch], frameon = False, loc='best', bbox_to_anchor=(1, 0., 0.5, 0.5))\n",
    "sns.despine()\n",
    "plt.xlabel(\"representations\")\n",
    "plt.ylabel(\"error rates\")\n",
    "plt.title(\"original & manipulated phonemes errors per layer\")\n",
    "print(plt.show())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### TASK 3 #######################\n",
    "\n",
    "def train_test_split(X, y):\n",
    "    I = (X.shape[0]//3)*2\n",
    "    X_train = X[:I, :]\n",
    "    y_train = y[:I]\n",
    "    X_val = X[I:,:]\n",
    "    y_val = y[I:]\n",
    "    return X_train, X_val, y_train, y_val\n",
    "\n",
    "import numpy as np\n",
    "def classifier(reps):\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    model = LogisticRegression(solver='sag', random_state=None,  multi_class='multinomial', warm_start=False, max_iter = 2000)\n",
    "    X = np.load(reps+\"_features.npy\")\n",
    "    y = np.load(reps+\"_phonemes.npy\")\n",
    "   \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    \n",
    "    model.fit(X_train,y_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    \n",
    "    DICTIONARY={reps:{}}\n",
    "    for letter in letters_separate(reps):\n",
    "        W = letters_separate(reps)[letter]['original'][1]\n",
    "        W = scaler.transform(W)\n",
    "        z = letters_separate(reps)[letter]['original'][0]\n",
    "        ori_err = 1-model.score(W, z)\n",
    "        \n",
    "        W1 = letters_separate(reps)[letter]['manipulated'][1]\n",
    "        W1 = scaler.transform(W1)\n",
    "        z1 = letters_separate(reps)[letter]['manipulated'][0]\n",
    "        mani_err = 1-model.score(W1, z1)\n",
    "    \n",
    "        prediction_manip = model.predict(W1)\n",
    "        prediction_orig = model.predict(W)\n",
    "        orig_class_probs = model.predict_proba(W)\n",
    "        manip_class_probs = model.predict_proba(W1)\n",
    "        \n",
    "        DICTIONARY[reps][letter]= [(ori_err, prediction_orig, orig_class_probs) ,\n",
    "                                   (mani_err, prediction_manip, manip_class_probs)]\n",
    "\n",
    "\n",
    "    return DICTIONARY\n",
    "    \n",
    "    \n",
    "  ###VIZUALIZATION####\n",
    "  \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "g = sns.FacetGrid(DF, col='phoneme', row='prediction', hue = 'prediction',height=4) #DF dataset of complied results shared in drive\n",
    "lbls = ['mfcc','conv','rec0','rec1','rec2','rec3']\n",
    "g.map(plt.scatter, \"representations\", \"predic_label_rate\", alpha=0.5,linewidth=0.5)\n",
    "plt.xticks(np.arange(6),lbls)\n",
    "fig = g.fig \n",
    "fig.set_size_inches(18, 8)\n",
    "fig.subplots_adjust(top=0.85, wspace=0.3)\n",
    "fig.suptitle('phoneme - representations - prediction label rate- prediction', fontsize=14)\n",
    "print(g.add_legend(title='manipulated phoneme prediction label'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
